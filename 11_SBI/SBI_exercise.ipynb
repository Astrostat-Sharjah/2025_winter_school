{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **SBI: In-class exercise**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Compatibility patch for GetDist + NumPy 2.0 ---\n",
    "# NumPy 2.0 removed np.infty, but older versions of GetDist still use it.\n",
    "# This patch restores it at runtime before importing GetDist.\n",
    "\n",
    "import numpy as np\n",
    "if not hasattr(np, 'infty'):\n",
    "    np.infty = np.inf  # Restore deprecated alias for compatibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "import numpy as np\n",
    "import emcee\n",
    "from scipy.stats import poisson\n",
    "from scipy.stats import chi2\n",
    "from scipy.optimize import basinhopping\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import pytorch_lightning as pl\n",
    "from scipy.stats import norm\n",
    "from getdist import plots, MCSamples\n",
    "from scipy.integrate import quad\n",
    "import multiprocessing\n",
    "from multiprocessing import Pool\n",
    "from nflows.flows.base import Flow\n",
    "from nflows.distributions.normal import StandardNormal\n",
    "from nflows.transforms.base import CompositeTransform\n",
    "from nflows.transforms.autoregressive import MaskedAffineAutoregressiveTransform\n",
    "from nflows.transforms.permutations import ReversePermutation\n",
    "from sbi.inference import SNPE  # old name used in v0.22.0\n",
    "from sbi.utils.torchutils import BoxUniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In the main notebook, our mock observations were generated by simply <span style=\"color:darkorange\">adding some Gaussian noise to the theoretical data</span>. However, in a <span style=\"color:darkorange\">real experiment</span>, our observations may be affected various <span style=\"color:darkorange\">other effects</span>, which may be caused by the instruments, the environment, physical processes, etc. These effects are often very **hard to model with a simple analytical function**, and may introduce additional **biases** in the data.\n",
    "\n",
    "In this exercise, we will assume that our observations are contaminated by two new effects: random instrumental drift and intermittent calibration errors.\n",
    "\n",
    "1. **Random instrumental drift**: The instrument that we use to measure the data is not perfect, and its <span style=\"color:darkorange\">calibration may drift over time</span> due to environmental factors or aging hardware. This drift will accumulate over time and so the measurements that we take at different times may be affected by a random offset. We can model this effect by adding a cumulative sum of normally distributed random values to the true magnitudes.\n",
    "    <details> \n",
    "    <summary>Why? [Click to expand]\n",
    "     <br><br>\n",
    "    </summary>\n",
    "    This simulates the effect of a drift that changes the measurements in a continuous but random manner. The random values are drawn from a normal distribution with a mean of zero, ensuring that the drift can go in either direction (positive or negative) and has a variance that reflects the magnitude of the drift per observation.\n",
    "    <br> <br>\n",
    "    </details>\n",
    "\n",
    "\n",
    "2. **Intermittent calibration errors**: The instrument may also suffer from occasional <span style=\"color:darkorange\">large calibration errors</span> that occur at random intervals. This may be caused by a variety of factors, such as power surges, cosmic rays, mechanical disturbances, miscalibrations during the data collection process etc. We model these errors by introducing occasional large spikes in the observed magnitudes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#random seed\n",
    "np.random.seed(1235)\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "# Constants\n",
    "c = 299792.458  # Speed of light in km/s\n",
    "\n",
    "# True cosmological parameters\n",
    "true_Omega_m = 0.3\n",
    "true_H0 = 70.0  # Hubble constant in km/s/Mpc\n",
    "\n",
    "# Redshifts of the supernovae\n",
    "z = np.linspace(0.01, 1.0, 50)\n",
    "\n",
    "# Noise level in the observed magnitudes\n",
    "sigma = 0.1\n",
    "\n",
    "# Luminosity distance function\n",
    "def luminosity_distance(z, Omega_m, H0):\n",
    "    \"\"\" Calculate the luminosity distance for a given redshift, Omega_m and H0 \"\"\"\n",
    "    integrand = lambda z_prime: 1.0 / np.sqrt(Omega_m * (1 + z_prime)**3 + (1 - Omega_m))\n",
    "    d_L = np.array([quad(integrand, 0, z_i)[0] for z_i in z])\n",
    "    return (c * (1 + z) * d_L) / H0\n",
    "\n",
    "# Generate synthetic data\n",
    "# Generate synthetic data with non-linear transformation\n",
    "d_L_true = luminosity_distance(z, true_Omega_m, true_H0)\n",
    "m_true = 5 * np.log10(d_L_true / 10)\n",
    "\n",
    "# Introduce random walk drift\n",
    "def random_walk_drift(m, z):\n",
    "    drift = np.cumsum(np.random.normal(0, 0.2, len(m)))\n",
    "    return m + drift\n",
    "\n",
    "# Introduce intermittent calibration errors\n",
    "def intermittent_calibration_errors(m, z):\n",
    "    errors = np.zeros_like(m)\n",
    "    error_indices = np.random.choice(len(m), size=int(len(m) * 0.1), replace=False)\n",
    "    errors[error_indices] = np.random.normal(0, 1, len(error_indices))\n",
    "    return m + errors\n",
    "\n",
    "# Apply both effects to the observed magnitudes\n",
    "m_obs_drift = random_walk_drift(m_true, z)\n",
    "m_obs = intermittent_calibration_errors(m_obs_drift, z) + np.random.normal(0, sigma, len(m_true))\n",
    "\n",
    "\n",
    "# Plot synthetic data\n",
    "plt.errorbar(z, m_obs, yerr=0.5, fmt='.', label='Observed magnitudes')\n",
    "plt.plot(z, m_true, label='True magnitudes')\n",
    "plt.xlabel('Redshift $z$')\n",
    "plt.ylabel('Apparent magnitude $m$')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\" style=\"margin-top: 20px\">\n",
    "\n",
    "# Part 1: Solving with Gaussian likelihood\n",
    "\n",
    "**Problem:** Given the observed magnitudes $m_{\\text{obs}}$ and redshifts $z$, infer the cosmological parameters $\\Omega_m$ and $H_0$ using the traditional Bayesian approach assuming a Gaussian likelihood:\n",
    "$$\\mathcal{L}(\\Omega_m, H_0) = \\prod_{i=1}^N \\frac{1}{\\sqrt{2\\pi\\sigma_i^2}} \\exp\\left(-\\frac{(m_{\\text{obs},i} - m_{\\text{th},i})^2}{2\\sigma_i^2}\\right), \\tag{3}$$\n",
    "\n",
    "**Tasks:**\n",
    "1. Define the prior, a Gaussian likelihood with some $\\sigma$ and the posterior.\n",
    "2. Run MCMC to sample from the posterior.\n",
    "3. Plot the corner plot of the posterior samples and see if the results are consistent with the true values.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger alertdanger\" style=\"margin-top: 20px\">\n",
    "\n",
    "<details>\n",
    "    <summary><b>Click to see the tip</b></summary>\n",
    "    \n",
    "Do ***exactly*** what was done in the main notebook:\n",
    "\n",
    "- For the prior, use a flat prior over $\\Omega_m$ and $H_0$.\n",
    "- For the Gaussian likelihood, you can assume a $\\sigma$ looking at the data (I would assume $\\sigma=0.5$, as in the main notebook).\n",
    "- For MCMC, you can use the `emcee` package.\n",
    "    \n",
    "</details>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log-prior function\n",
    "def log_prior(params):\n",
    "    Omega_m, H0 = params\n",
    "    if ...\n",
    "        return ...\n",
    "    else:\n",
    "        return ...\n",
    "    \n",
    "# Define the log-likelihood function\n",
    "def log_like(params, z, m_obs):\n",
    "    Omega_m, H0 = params\n",
    "    d_L = ...\n",
    "    m_th = ...\n",
    "    sigma_m = ...  # our assumed standard deviation of magnitudes\n",
    "    return ... \n",
    "\n",
    "# Log-posterior function\n",
    "def log_post(params, z, m_obs):\n",
    "    lp = log_prior(...)\n",
    "    if not np.isfinite(lp):\n",
    "        return -np.inf # If prior is -inf, return directly -inf\n",
    "    else:\n",
    "        return lp + log_like(params, z, m_obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampling with `emcee` \n",
    "ndim, nwalkers = ... , ...\n",
    "ncpus = multiprocessing.cpu_count()\n",
    "\n",
    "# Initialize walkers \n",
    "pos = ...\n",
    "\n",
    "with Pool(ncpus) as pool:\n",
    "    sampler = emcee.EnsembleSampler(nwalkers, ndim, log_post, args=(z, m_obs), pool=pool)\n",
    "    sampler_output = sampler.run_mcmc(pos, nsteps=..., progress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get samples and convert to GetDist MCSamples\n",
    "flat_samples = sampler.get_chain(discard=..., flat=True)\n",
    "\n",
    "# GetDist plotting\n",
    "samples = MCSamples(samples=flat_samples, names=['\\Omega_m', 'H_0'], labels=['\\Omega_m', 'H_0'])\n",
    "g = plots.get_subplot_plotter(width_inch=4)\n",
    "g.triangle_plot(samples, filled=True, markers={'\\Omega_m': true_Omega_m, 'H_0': true_H0})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\" style=\"margin-top: 20px\">\n",
    "\n",
    "# Part 2: Solving with Neural Posterior Estimation\n",
    "\n",
    "**Problem:** Given the observed magnitudes $m_{\\text{obs}}$ and redshifts $z$, infer the cosmological parameters $\\Omega_m$ and $H_0$ using the Neural Posterior Estimation approach.\n",
    "\n",
    "**Tasks:**\n",
    "1. Construct a **simulator** that generates the observed magnitudes $m_{\\text{obs}}$ given the cosmological parameters $\\Omega_m$ and $H_0$. Encode the random instrumental drifts and intermittent calibration errors in the simulator.\n",
    "2. Generate a **training dataset** of observed magnitudes $m_{\\text{obs}}$ and corresponding cosmological parameters $\\Omega_m$ and $H_0$, by running the simulator for a range of randomly drawn parameters $\\Omega_m$ and $H_0$. \n",
    "3. Define the **Neural Posterior Estimator** class using the `sbi` package that will estimate the posterior with a normalizing flow.\n",
    "4. Sample from the NPE, conditioning on the observed magnitudes $m_{\\text{obs}}$ that we had in the beginning.\n",
    "5. Plot the corner plot of the posterior samples and see if the results are consistent with the true values.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger alertdanger\" style=\"margin-top: 20px\">\n",
    "\n",
    "<details>\n",
    "    <summary><b>Click to see the tip</b></summary>\n",
    "\n",
    "Again, do ***exactly*** what was done in the main notebook. The actual changes that you need to implement are:\n",
    "* In the simulator, introduce the random instrumental drifts and intermittent calibration errors.\n",
    "* For the test data, use instead the observed magnitudes $m_{\\text{obs}}$ that we showed in the beginning.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward model to simulate observed magnitudes\n",
    "def luminosity_distance_simulator(params, z):\n",
    "    Omega_m, H0 = params\n",
    "    d_L = ...\n",
    "    m_th = ...\n",
    "    m_drift = ... # add the random walk drift here\n",
    "    m_drift_calibration = ... # add the intermittent calibration errors here\n",
    "    m_sim = ... # add the standard noise here, to get the final simulated magnitudes\n",
    "    \n",
    "    return ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1234) # use random seed for reproducibility\n",
    "\n",
    "# Generate training data\n",
    "n_train = 20_000\n",
    "param_samples = np.random.uniform(low=[0, 50], high=[1, 100], size=(n_train, 2)) # Parameter proposal\n",
    "x_samples = np.array([luminosity_distance_simulator(theta, z) for theta in tqdm(param_samples)])\n",
    "\n",
    "# Convert to tensors\n",
    "param_samples = ...\n",
    "x_samples = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior = ... \n",
    "inference = ...\n",
    "inference.append_simulations(...)\n",
    "density_estimator = ...\n",
    "posterior = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tasks 4 & 5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_npe = ...\n",
    "\n",
    "# Convert to numpy (ensure on CPU and detached from graph)\n",
    "if isinstance(samples_npe, torch.Tensor):\n",
    "    samples_npe = samples_npe.detach().cpu().numpy()\n",
    "else:\n",
    "    samples_npe = np.asarray(samples)\n",
    "\n",
    "\n",
    "# Create a getdist MCSamples object\n",
    "gd_samples_npe = MCSamples(samples=samples_npe, labels=['\\Omega_m', 'H_0'], names=['\\Omega_m', 'H_0'])\n",
    "\n",
    "# Make a triangle/contour plot with getdist\n",
    "g = plots.get_subplot_plotter()\n",
    "g.triangle_plot([gd_samples_npe, samples], filled=True, markers={'\\Omega_m': true_Omega_m, 'H_0': true_H0}, legend_labels=['NPE', 'MCMC'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "astrostat24",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
