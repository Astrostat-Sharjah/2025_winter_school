{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=6>**DL Regression**</font> </h6>\n",
    "\n",
    "<div style=\"border: 1px solid lightgray; padding: 10px; background-color: #f9f9f9;\">\n",
    "\n",
    "<font size=1>\n",
    "\n",
    "    Winter School for AstroStatistics in Sharjah 2025\n",
    "\n",
    "The content presented in this notebook is the original work of the authors, unless specified otherwise.\n",
    "Any publicly available material incorporated is properly credited to its respective sources.\n",
    "All references to published papers, datasets, and software tools are duly acknowledged.\n",
    "The original content of this notebook is licensed under the GNU General Public License v3.0 (GNU GPLv3).\n",
    "</font>\n",
    "</div>\n",
    "\n",
    "<hr style=\"height:0.5px; border:none; color:lightgray; background-color:lightgray;\">\n",
    "\n",
    "In this session we provide a short introduction to **Regression**, a topic which can be covered by both machine learning applications (such as those in [scikit-learn](https://scikit-learn.org/stable/index.html) we saw in clustering and classification session) and deep learning approaches. <br>\n",
    "The goals are:\n",
    "\n",
    "- to get an idea of how we **build and train** a deep learning model\n",
    "- explore some **basic concepts** to better understand their use and impact. \n",
    "\n",
    "In the example that will follow we are mainly going through these steps:\n",
    "\n",
    "    1. Load and Select Data\n",
    "    2. Define Model\n",
    "    3. Compile Model\n",
    "    4. Fit Model \n",
    "    5. Iterate steps 2-3-4 (by adjusting various parameters or the model architecture)\n",
    "    6. Evaluate Model\n",
    "    7. Make Predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# Set up a fancy plot style (you can comment it out without consequences)\n",
    "import sys; sys.path.append('../src'); import plot_style\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "import time\n",
    "\n",
    "import keras\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense, Input, BatchNormalization,Conv3D, MaxPooling3D, Dense, Add, Activation\n",
    "from keras.regularizers import l2\n",
    "from keras.models import Model, Sequential\n",
    "from keras.optimizers import Adam, SGD, Adagrad, RMSprop\n",
    "\n",
    "from astropy.io import fits\n",
    "from astropy.table import Table\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "from IPython.display import clear_output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is regression ?\n",
    "\n",
    "    Modeling problems where the output is a continuous numeric value.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear regression and least squares\n",
    "\n",
    "A linear regression model will try to predict the single value of the dependent variable $y$ given the independent variable $x$, with the most generic form being: \n",
    "\n",
    "$$y = f (x | β), $$\n",
    "\n",
    "where $x$ corresponds to the input variable(s) and $β$ is an array of parameters.\n",
    "\n",
    "The simplest linear model we can think of is a line:\n",
    "\n",
    "$$y = β_0 + β_1 x $$\n",
    "\n",
    "where $β_0$ and $β_1$ have the usual meaning of intercept and slope of a line. \n",
    "\n",
    "Generalizing a bit, for an input vector $x^T = (x_1, x_2, ..., x_N)$,  where $N$ is the total number of observations (or samples), the linear model to predict the real-valued output $y$ is:\n",
    "\n",
    "$$ f(x) = β_0 + \\sum_{i=1}^{N} x_i β_i. $$\n",
    "\n",
    "To find these parameters we use the Ordinary Least Squares approach, i.e. we try to **minimize** the residual sum of the squares between the observations and the predictions by the model (i.e. the **loss or cost** function):\n",
    "\n",
    "$$ RSS(β) = \\sum_{i=1}^{N} (y_i - f(x_i))^2 =  \\sum_{i=1}^{N} (y_i - β_0 - x_i β_i)^2 .$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The more general linear regression\n",
    "\n",
    "Linear regression refers to modeling functions that are linear with respect to the parameters (coefficients) and not with respect to the variables! For example, the function:\n",
    "\n",
    "$$f (x|β) = \\sum_{i=1}^N β_i g_i(x) = β_1 g_1(x) + β_2 g_2(x)~+~...~+~β_N g_N(x) $$\n",
    "\n",
    "describes a linear problem as long as the sub-functions $g_i(x)$ do not depend on any of the parameters $β_i$. This is not the most generic formulation of the linear regression but we are going to use this form in the following applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# On metrics ... or how well can we do\n",
    "\n",
    "> _Accuracy is a measure for classification not regression_<br>\n",
    "> _We cannot calculate accuracy for a regression model_\n",
    ">\n",
    ">  _by Jason Brownlee [Regression Metrics for Machine Learning](https://machinelearningmastery.com/regression-metrics-for-machine-learning/)_\n",
    "\n",
    "In regression we are dealing with continuous values. Therefore, it is actually impossible to predict the exact same values. The idea is to get an estimate of how close the predictions are to the expected values.\n",
    "\n",
    "We are going to refer only a few of the available metrics in [sklearn](https://machinelearningmastery.com/regression-metrics-for-machine-learning/). In the following $y$ refers to the dependent values while $\\hat{y}$ to the predicted values.\n",
    "\n",
    "**--> Mean Squared Error (MSE)**\n",
    "\n",
    "$$ MSE = \\frac{1}{N}\\sum_{i=1}^{N} (y_i - \\hat{y_i})^2 $$  \n",
    "\n",
    "It is actually the cost function of the Ordinary Least Squares (check $RSS(β)$ above). The units returned in this case are squared. Best score is 0.\n",
    "\n",
    "**--> Root Mean Squared Error (RMSE)**\n",
    "\n",
    "$$ RMSE = \\sqrt{ \\frac{1}{N}\\sum_{i=1}^{N} (y_i - \\hat{y_i})^2 } $$  \n",
    "\n",
    "It returns the square root of MSE so that the units match the units of the target value (so better interpretation).  Best score is 0.\n",
    "\n",
    "**--> Mean Absolute Error (MAE)**\n",
    "\n",
    "$$ MAE = {1 \\over N}\\sum_i^N{|  y_i-\\hat{y_i} |}$$  \n",
    "\n",
    "It is less sensitive to large errors when compared to (R)MSE. The score is in units of the target value.\n",
    "\n",
    "> Comment: Although arithetically the best scores for (R)MSE and MAE is 0 this cannot be the case in real-life problems. Instead a baseline model has to be determined and calculate its score. Then, any model that can achieve a score better that the baselie model is accepted as a skilful model.  \n",
    "\n",
    "\n",
    "**--> R2 (coefficient of determination)**\n",
    "\n",
    "$$R^2 = 1 - {\\sum_{i=1}^N{(y_i-\\hat{y_i})^2} \\over \\sum_{i=1}^N{(y_i - \\bar{y})^2}}, $$ \n",
    "\n",
    "where $\\bar{y} = \\frac{1}{n}\\sum_{i=1}^N y_i$. \n",
    "\n",
    "It represents the proportion of variance (of y) that has been explained by the independent variables in the model. It provides an indication of goodness of fit and therefore a measure of how well unseen samples are likely to be predicted by the model, through the proportion of explained variance. Best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Application 1: Estimate SFR\n",
    "\n",
    "We will use data derived from the Heraklion Extragalactic Catalogue (HECATE; [Kovlakas et al, 2021](https://ui.adsabs.harvard.edu/abs/2021MNRAS.506.1896K/abstract)) which is an all-sky galaxy catalogue, containing about 200k galaxies (up to z=0.047, D≲200Mpc), and it offers positions, sizes, distances, morphological classifications, star formation rates, stellar masses, metallicities, and nuclear activity classifications. \n",
    "\n",
    "In particular, we are going to use a dataset from the work of [Kouroumpatzakis et al. 2023](https://ui.adsabs.harvard.edu/abs/2023A%26A...673A..16K/abstract) where they estimate the Star Formation Rates (SFR) and stellar mass using model Spectral Enerny Distributions with a variety of parameters (IR photometry, extinction, and stellar populations). \n",
    "\n",
    "<div class=\"alert alert-warning\" role=\"alert\" style=\"border-radius: 8px; padding: 10px\">\n",
    "  \n",
    "<font size=4>**Exercise 1:**</font>    \n",
    "\n",
    "**Objective:** Build a regression model to predict SFR\n",
    "\n",
    "**Task:** For this follow these steps:\n",
    "    \n",
    "- load and investigate the data\n",
    "    \n",
    "- select features\n",
    "    \n",
    "- pre-process the data\n",
    "    \n",
    "- split data (train/validation/test)\n",
    "    \n",
    "- build model (architecture with dense layers)\n",
    "    \n",
    "- train the model\n",
    "    \n",
    "- evaluate its performance\n",
    "    \n",
    "- compare with baseline model and check results.    \n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and checking data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# import time\n",
    "\n",
    "# import keras\n",
    "# from keras.layers import Activation, Dropout, Flatten, Dense, Input, BatchNormalization,Conv3D, MaxPooling3D, Dense, Add, Activation\n",
    "# from keras.regularizers import l2\n",
    "\n",
    "# from keras.models import Model, Sequential\n",
    "# from keras.optimizers import Adam, SGD, Adagrad, RMSprop\n",
    "# from sklearn.metrics import r2_score\n",
    "\n",
    "# from IPython.display import clear_output\n",
    "\n",
    "class PlotLosses(keras.callbacks.Callback):\n",
    "    # NOTE: the current version can print only the first metric from the list provided\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.i = 0\n",
    "        self.x = []\n",
    "        self.losses = []\n",
    "        self.val_losses = []\n",
    "        self.losses2 = []\n",
    "        self.val_losses2 = []\n",
    "        \n",
    "        self.fig = plt.figure()\n",
    "        \n",
    "        self.logs = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        self.logs.append(logs)\n",
    "        self.x.append(self.i)\n",
    "        self.losses.append(logs.get('loss'))\n",
    "        self.val_losses.append(logs.get('val_loss'))\n",
    "        self.losses2.append(logs.get( test_metrics[0] ) )\n",
    "        self.val_losses2.append(logs.get( f'val_{test_metrics[0]}' ))\n",
    "#         print(test_metrics[0] , logs.get( test_metrics[0] ), logs.get( f'val_{test_metrics[0]}' ))\n",
    "#         print(self.losses2)\n",
    "\n",
    "        self.i += 1\n",
    "        \n",
    "        clear_output(wait=True)\n",
    "        plt.subplot(1,2,1)\n",
    "        plt.plot(self.x, self.losses2, label=\"Train\",linestyle='-')\n",
    "        plt.plot(self.x, self.val_losses2, label=\"Validate\",linestyle='--')\n",
    "#        plt.ylim(0,1)\n",
    "        plt.legend()\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel(test_metrics[0])\n",
    "        \n",
    "        plt.subplot(1,2,2)\n",
    "        plt.plot(self.x, self.losses, label=\"Train\",linestyle='-')\n",
    "        plt.plot(self.x, self.val_losses, label=\"Validate\",linestyle='--')\n",
    "\n",
    "        plt.legend()\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        plt.show();\n",
    "        \n",
    "plot_losses = PlotLosses()\n",
    "\n",
    "\n",
    "def sub_ploots(plots_cols, plots_array):\n",
    "        \"\"\" Automatic adjustment of subplots\n",
    "        Given the \n",
    "        plots_cols     : plots per row (set manually)\n",
    "        plots_array    : array from which the number of \n",
    "                         individual subplots is derived\n",
    "\n",
    "        the output is given as \n",
    "        plots_cols     : plots per row\n",
    "        plots_rows     : number of necessary rows to create\n",
    "        \"\"\"\n",
    "        plots_unique = len((list(set(plots_array))))\n",
    "        plots_rows = int(np.ceil(plots_unique/plots_cols))\n",
    "        \n",
    "        return plots_rows, plots_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# following this tutorial :https://learn.astropy.org/tutorials/FITS-tables.html\n",
    "\n",
    "data_fits = fits.open('data/CIGALE_SFR.fits', memmap=True)\n",
    "\n",
    "# printing some information \n",
    "data_fits.info()\n",
    "print('-----')\n",
    "print(data_fits[1].columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note on columns:**\n",
    "\n",
    "```\n",
    "       'sfr' : Star Formation Rate\n",
    "     'mstar' : Stellar Mass\n",
    "       'IRX' : IR excess ratio \n",
    "      'EBVs' : extinction\n",
    " 'LYoungOld' : ratin of luminosity of young to old populations\n",
    "   'NIR200' : JWST NIR-F200W\n",
    " 'MIRI2100' : JWST MIRI-F2100W\n",
    "       'W1' : WISE 3.4 μm \n",
    "       'W2' : WISE 4.6 μm \n",
    "       'W3' : WISE 12 μm \n",
    "       'W4' : WISE 22 μm \n",
    "```\n",
    "\n",
    "Printing the data table. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tab = Table(data_fits[1].data)\n",
    "data_tab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting features\n",
    "\n",
    "In this part we can select which features we want to keep and use to train our model. Feel free to choose any number of feature - keep in mind though **NOT** to select `sfr` and `mstar` as these are needed for the output (i.e. are the values we want to predict).\n",
    "\n",
    "\n",
    "<div class=\"alert alert-warning\" role=\"alert\" style=\"border-radius: 8px; padding: 10px\">\n",
    "  \n",
    "**Task:** select the features to work with\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sel_features = [ ... ] \n",
    "\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "\n",
    "rws, cls = sub_ploots(2, sel_features)  # for subplots\n",
    "for f in range(len(sel_features)):\n",
    "    f_name = sel_features[f]\n",
    "    ax = fig.add_subplot(rws, cls, f+1)\n",
    "    ax.set_title(f'Feature {f_name}')\n",
    "    ax.hist(data_tab[f_name], bins='auto', align='mid', density=True)\n",
    "    ax.set_xlabel('feature value')\n",
    "    ax.set_ylabel('counts or probability density')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data_tab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and the data table with the _selected features_ looks like ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tab[sel_features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We convert table objects into numpy arrays:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = data_tab[sel_features].as_array()\n",
    "values = values.view((float, len(values.dtype.names)))\n",
    "target = np.array(data_tab['sfr']).reshape(-1,1)\n",
    "\n",
    "print('The values for selected features:')\n",
    "print(values)\n",
    "print()\n",
    "print('The target quantity:')\n",
    "print(target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the sample and ... ?\n",
    "\n",
    "<div class=\"alert alert-warning\" role=\"alert\" style=\"border-radius: 8px; padding: 10px\">\n",
    "  \n",
    "**Tasks:** \n",
    "\n",
    "- split the sample into train/validation/test\n",
    "\n",
    "- what should you do before training the model?\n",
    "    \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# keep aside the test set first (coming from the future!)\n",
    "X_train_full, X_test_un, y_train_full, y_test = train_test_split( ..., ... ,\n",
    "                        test_size= ... ) #, random_state=42) \n",
    "\n",
    "# split the remaining sample intro train and validation \n",
    "X_train_un, X_valid_un, y_train, y_valid = train_test_split( ... , ... , \n",
    "                        test_size= ... ) #, random_state=42) \n",
    "\n",
    "scaler = ...\n",
    "X_train = scaler.fit_transform(...)\n",
    "X_valid = scaler.transform( ... )\n",
    "X_test = scaler.transform( ... )\n",
    "\n",
    "print(f'> From {len(target)} sources:')\n",
    "print(f'   {len(X_train)} (train)')\n",
    "print(f'   {len(X_valid)} (validation)')\n",
    "print(f'   {len(X_test)} (test)') \n",
    "print('\\n> Statistics per feature:')\n",
    "print(f'Mean: {scaler.mean_}')\n",
    "print(f'Variance: {scaler.var_}')\n",
    "print()\n",
    "print(f' Target shape: {y_train.shape}') # print the shape of the output values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time to build our model! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you want you can add more output metrics!\n",
    "# but keep in mind that the plot_losses will use the first one.\n",
    "\n",
    "test_metrics = [ 'mean_squared_error', 'mean_absolute_error' ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\" role=\"alert\" style=\"border-radius: 8px; padding: 10px\">\n",
    "  \n",
    "**Tasks:** \n",
    "\n",
    "- build a DL network with a number of dense layers - you can add as many layers as you want (probably you may need to return to correct this).  \n",
    "\n",
    "- how many nodes the last layer should have?\n",
    "    \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add( Dense( ... , input_shape=X_train_full.shape[1:], activation='relu') )\n",
    "\n",
    "model.add( Dense( ..., activation='relu') )\n",
    "...\n",
    "\n",
    "model.add( Dense(...) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optzr = Adam(learning_rate=1e-4) \n",
    "model.compile(loss='mean_squared_error', optimizer=optzr, \n",
    "              metrics = test_metrics) \n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finally ... train the model!\n",
    "\n",
    "<div class=\"alert alert-warning\" role=\"alert\" style=\"border-radius: 8px; padding: 10px\">\n",
    "  \n",
    "**Task:** Fill the missing code to train the model\n",
    "    \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "start_time = time.time() \n",
    "\n",
    "history=model.fit( ... , ... , \n",
    "                    batch_size= ... , \n",
    "                    epochs= ... ,\n",
    "                    validation_data=[ ... , ... ],\n",
    "                    callbacks=[plot_losses],shuffle=True)\n",
    "\n",
    "# to print history contents\n",
    "# history.history\n",
    "elapsed_time = time.time() - start_time\n",
    "time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ... and the moment of truth - evaluation\n",
    "\n",
    "<div class=\"alert alert-warning\" role=\"alert\" style=\"border-radius: 8px; padding: 10px\">\n",
    "  \n",
    "**Task:** evaluate the model (take care on which dataset!) \n",
    "    \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation = model.evaluate( ... , ... )\n",
    "print(f\"Loss value: {evaluation[0]:.2f}\")  \n",
    "for m in range(len(test_metrics)):\n",
    "    print(f\"{test_metrics[m]}: {evaluation[m+1]:0.2f}\")  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing with a baseline model...\n",
    "\n",
    "<div class=\"alert alert-warning\" role=\"alert\" style=\"border-radius: 8px; padding: 10px\">\n",
    "  \n",
    "**Task:** fill in the code to train and evaluate the baseline model.\n",
    "    \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model = LinearRegression()\n",
    "lr_model.fit( ... , ... )\n",
    "y_hat_lr = lr_model.predict( ... )\n",
    "\n",
    "mse = mean_squared_error( ... , ... )\n",
    "mae = mean_absolute_error( ... , ... )\n",
    "r2 = r2_score( ... , ...)\n",
    "\n",
    "print(f\"MSE: {mse:.2f}\")\n",
    "print(f\"MAE: {mae:.2f}\")\n",
    "print(f\"R2 : {r2:.2f}\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking the predicitons\n",
    "\n",
    "<div class=\"alert alert-warning\" role=\"alert\" style=\"border-radius: 8px; padding: 10px\">\n",
    "  \n",
    "**Task:** fill in the code to check the results.\n",
    "    \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = model.predict( ... )    \n",
    "\n",
    "r2 = r2_score( ... , ... )\n",
    "\n",
    "min_ax, max_ax = np.min(y_test), np.max(y_test)\n",
    "\n",
    "plt.plot(y_test, y_hat, '.', alpha=0.4)\n",
    "plt.title( f'R2 = {r2:0.2f}', fontsize=20)\n",
    "plt.xlabel('true', fontsize=16)\n",
    "plt.ylabel('prediction', fontsize=16)\n",
    "plt.xlim([min_ax, max_ax])\n",
    "\n",
    "# 1-to-1 line:\n",
    "xx = np.linspace(min_ax, max_ax, 100)\n",
    "plt.plot(xx, xx, c=\"grey\", linestyle='--')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Application 2: Estimate SFR and stellar mass\n",
    "\n",
    "<div class=\"alert alert-warning\" role=\"alert\" style=\"border-radius: 8px; padding: 10px\">\n",
    "  \n",
    "<font size=4>**Exercise 2:**</font>    \n",
    "\n",
    "**Objective:** Build a regression model to predict both SFR ('sfr') and stellar mass ('mstar').\n",
    "\n",
    "**Task:** Follow the same steps with the previous exercise. You need to adjust the model to train and predict both values.\n",
    "\n",
    "</div>\n",
    "\n",
    "\n",
    "In this case we want to use all/part of the available features to estimate both the SFR (`sfr`) **and** the stellar mass (`mstar`). \n",
    "\n",
    "Reminder of the available features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_fits[1].columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\" role=\"alert\" style=\"border-radius: 8px; padding: 10px\">\n",
    "  \n",
    "**Task:** Select the features to work with. The features we want to predict now are two, the SFR and the stellar mass.\n",
    "    \n",
    "</div>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sel_features = [ ... ]\n",
    "pre_features = [ ... ]\n",
    "\n",
    "values = data_tab[sel_features].as_array()\n",
    "values = values.view((float, len(values.dtype.names)))\n",
    "\n",
    "target = data_tab[pre_features].as_array()\n",
    "target = target.view((float, len(target.dtype.names)))\n",
    "\n",
    "\n",
    "print('The values for selected features:')\n",
    "print(values)\n",
    "print()\n",
    "print('The target quantity:')\n",
    "print(target)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\" role=\"alert\" style=\"border-radius: 8px; padding: 10px\">\n",
    "  \n",
    "**Tasks:** \n",
    "\n",
    "- split the sample into train/validation/test\n",
    "\n",
    "- what should you do before training the model?\n",
    "    \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep aside the test set first (coming from the future!)\n",
    "X_train_full, X_test_un, y_train_full, y_test = train_test_split(... ,...,\n",
    "                        test_size= ...) #, random_state=42) \n",
    "\n",
    "# split the remaining sample intro train and validation \n",
    "X_train_un, X_valid_un, y_train, y_valid = train_test_split( ... , ..., \n",
    "                        test_size= ...) #, random_state=42) \n",
    "\n",
    "scaler = ...\n",
    "X_train = scaler.fit_transform( ... )\n",
    "X_valid = scaler.transform( ... )\n",
    "X_test = scaler.transform( ... )\n",
    "\n",
    "print(f'> From {len(target)} sources:')\n",
    "print(f'   {len(X_train)} (train)')\n",
    "print(f'   {len(X_valid)} (validation)')\n",
    "print(f'   {len(X_test)} (test)') \n",
    "print('\\n> Statistics per feature:')\n",
    "print(f'Mean: {scaler.mean_}')\n",
    "print(f'Variance: {scaler.var_}')\n",
    "print()\n",
    "print(f' Target shape: {y_train.shape}') # print the shape of the output values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the metrics again here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you want to add more output metrics\n",
    "test_metrics = ['mean_squared_error', 'mean_absolute_error', ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"alert alert-warning\" role=\"alert\" style=\"border-radius: 8px; padding: 10px\">\n",
    "  \n",
    "**Task:** Building our new model. Watch out the number of nodes at the very last step!\n",
    "    \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add( Dense(,,,, input_shape=X_train.shape[1:], activation='relu') )\n",
    "\n",
    "model.add( Dense(..., activation='relu') )\n",
    "...\n",
    "\n",
    "model.add( Dense(...) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optzr =  Adam(learning_rate=1e-4)\n",
    "model.compile(loss= 'mean_squared_error',  # custom_loss, \n",
    "              optimizer=optzr, \n",
    "              metrics = test_metrics)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\" role=\"alert\" style=\"border-radius: 8px; padding: 10px\">\n",
    "  \n",
    "**Task:** Train the model\n",
    "    \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "start_time = time.time() \n",
    "\n",
    "history=model.fit( ... , ... , \n",
    "                    batch_size= ..., \n",
    "                    epochs= ...,\n",
    "                    validation_data=[ ... , ...],\n",
    "                    callbacks=[plot_losses],shuffle=True)\n",
    "\n",
    "# to print history contents\n",
    "# history.history\n",
    "elapsed_time = time.time() - start_time\n",
    "time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\" role=\"alert\" style=\"border-radius: 8px; padding: 10px\">\n",
    "  \n",
    "**Task:** Compare with baseline model.\n",
    "    \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model = LinearRegression()\n",
    "lr_model.fit( ... , ...)\n",
    "y_hat_lr = lr_model.predict( ... )\n",
    "\n",
    "mse_lr = mean_squared_error(  ... , ...)\n",
    "mae_lr = mean_absolute_error( ... , ...)\n",
    "r2_lr = r2_score(y_test, y_hat_lr, multioutput='raw_values')\n",
    "print(f\"Mean Squared Error (MSE): {mse_lr:.2f}\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae_lr:.2f}\")\n",
    "print(f\"R2 (sfr): {r2_lr[0]:0.2f} \")\n",
    "print(f\"R2 (mstar): {r2_lr[1]:0.2f} \")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"alert alert-warning\" role=\"alert\" style=\"border-radius: 8px; padding: 10px\">\n",
    "  \n",
    "**Task:** check the results.\n",
    "    \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation = model.evaluate( ... , ...)\n",
    "print(f\"Loss value: {evaluation[0]:.2f}\")  \n",
    "for m in range(len(test_metrics)):\n",
    "    print(f\"{test_metrics[m]}: {evaluation[m+1]:0.2f}\")  \n",
    "\n",
    "print('------')\n",
    "\n",
    "y_pred = model.predict( ... ) \n",
    "\n",
    "fig, axes = plt.subplots(1,2, figsize=(12,6))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, ax in enumerate(axes):\n",
    "  \n",
    "    r2 = r2_score( y_test[:,i] , y_pred[:,i])\n",
    "    print(i, r2)\n",
    "    \n",
    "    min_ax, max_ax = np.min(y_test[:,i]), np.max(y_test[:,i])\n",
    "\n",
    "    ax.plot(y_test[:,i], y_pred[:,i], '.', alpha=0.4)\n",
    "    ax.set_title( f'R2 = {r2:0.2f} for {pre_features[i]}', fontsize=20)\n",
    "    ax.set_xlabel('true', fontsize=16)\n",
    "    ax.set_ylabel('prediction', fontsize=16)\n",
    "    ax.set_xlim([min_ax, max_ax])\n",
    "\n",
    "    min_ax, max_ax = np.min(y_test[:,i]), np.max(y_test[:,i])\n",
    "    # 1-to-1 line:\n",
    "    xx = np.linspace(min_ax, max_ax, 100)\n",
    "    ax.plot(xx, xx, c=\"grey\", linestyle='--')\n",
    "    \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EOF"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "astrostat25",
   "language": "python",
   "name": "astrostat25"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "409.6px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
